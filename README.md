# ğŸ­ Projet Data Warehouse ETL

## ğŸ“‹ Vue d'ensemble

Ce projet implÃ©mente un entrepÃ´t de donnÃ©es complet avec un pipeline ETL (Extract, Transform, Load) pour analyser des donnÃ©es de ventes. Le systÃ¨me extrait les donnÃ©es depuis des fichiers sources, les transforme selon un modÃ¨le en Ã©toile, et les charge dans une base de donnÃ©es PostgreSQL pour permettre des analyses avancÃ©es.

## âœ¨ FonctionnalitÃ©s principales

- ğŸ“¥ **Pipeline ETL robuste** : Extraction, transformation et chargement des donnÃ©es
- ğŸ“Š **Analyses automatisÃ©es** : GÃ©nÃ©ration de graphiques et rapports sur les ventes
- ğŸ“ˆ **Tableaux de bord** : Visualisation interactive des indicateurs clÃ©s
- ğŸ”„ **Mise Ã  jour planifiÃ©e** : Actualisation automatique des donnÃ©es
- ğŸ“ **Journalisation dÃ©taillÃ©e** : Suivi complet des opÃ©rations

## ğŸ—ï¸ Structure du projet

```
myproject_data_warehouse/
â”œâ”€â”€ config/              # Fichiers de configuration
â”œâ”€â”€ data/                # DonnÃ©es sources et gÃ©nÃ©rÃ©es
â”œâ”€â”€ dashboard/           # Interface de visualisation 
â”œâ”€â”€ db/                  # Scripts de base de donnÃ©es
â”œâ”€â”€ docs/                # Documentation dÃ©taillÃ©e
â”œâ”€â”€ logs/                # Journaux d'exÃ©cution
â”œâ”€â”€ results/             # RÃ©sultats d'analyses
â”‚   â””â”€â”€ visualisations/  # Graphiques et rapports gÃ©nÃ©rÃ©s
â”œâ”€â”€ scripts/             # Scripts d'analyse et utilitaires
â”œâ”€â”€ src/                 # Code source principal
â”‚   â”œâ”€â”€ etl/             # Composants du pipeline ETL
â”‚   â”œâ”€â”€ utils/           # Fonctions utilitaires
â”‚   â””â”€â”€ visualization/   # GÃ©nÃ©rateurs de visualisations
â””â”€â”€ run.py               # Point d'entrÃ©e principal
```

## ğŸš€ Installation et dÃ©marrage

### PrÃ©requis

- ğŸ Python 3.8+
- ğŸ˜ PostgreSQL 12+
- ğŸ“¦ Packages Python (voir `requirements.txt`)

### Installation

1. **Cloner le dÃ©pÃ´t**

```bash
git clone [url-du-dÃ©pÃ´t]
cd myproject_data_warehouse
```

2. **Installer les dÃ©pendances**

```bash
pip install -r requirements.txt
```

3. **Configurer la base de donnÃ©es**

Assurez-vous que PostgreSQL est en cours d'exÃ©cution et crÃ©ez une base de donnÃ©es nommÃ©e `data_warehouse`.

4. **Configuration**

Modifiez les paramÃ¨tres de connexion Ã  PostgreSQL dans `src/utils/config.yaml` si nÃ©cessaire:

```yaml
database:
  user: 'postgres'
  password: 'admin'  # Ã€ modifier selon votre configuration
  host: 'localhost'
  port: '5432'
  name: 'data_warehouse'
```

## ğŸ“Œ Architecture du pipeline ETL

### ğŸ§© Vue d'ensemble de l'architecture

Notre architecture ETL suit un modÃ¨le minimaliste et se concentre sur les fonctionnalitÃ©s essentielles :

1. **ğŸ“¥ Extraction** : Lecture des fichiers CSV sources de faÃ§on robuste
2. **âš™ï¸ Transformation** : Conversions et nettoyage des donnÃ©es
3. **ğŸ“¤ Chargement** : Insertion dans PostgreSQL avec gestion d'erreurs

### ğŸ”„ Flux de donnÃ©es

Le processus ETL suit ce flux de donnÃ©es:

1. ğŸ“¥ **Extraction** : Lecture des fichiers CSV depuis le dossier `data/`
   - ğŸ‘¥ `customers_dim.csv`: DonnÃ©es des clients
   - ğŸ“¦ `products_dim.csv`: DonnÃ©es des produits
   - ğŸ¬ `stores_dim.csv`: DonnÃ©es des magasins
   - ğŸ“… `time_dim.csv`: Dimension temporelle
   - ğŸ’° `sales_fact.csv`: Table de faits des ventes

2. âš™ï¸ **Transformation**
   - ğŸ§¹ **Nettoyage** : Suppression des espaces, conversion des types, gestion des valeurs manquantes
   - âœ… **Validation** : DÃ©tection des doublons, correction des valeurs nÃ©gatives, vÃ©rification de cohÃ©rence
   - ğŸ” **IntÃ©gritÃ©** : Validation des clÃ©s Ã©trangÃ¨res, suppression des enregistrements invalides

3. ğŸ“¤ **Chargement**
   - ğŸ—„ï¸ CrÃ©ation du schÃ©ma en Ã©toile dans PostgreSQL
   - ğŸ“Š Insertion dans les tables dimensionnelles (Customers, Products, Stores, Time)
   - ğŸ“ˆ Insertion dans la table de faits (Sales)
   - ğŸ” CrÃ©ation d'index pour optimiser les performances

### ğŸ§© Composants Principaux

- ğŸ”„ **Pipeline ETL** (`src/etl/pipeline.py`) : Classe `SimpleETL` qui encapsule la logique du pipeline
- ğŸ’¾ **Chargeur SQL** (`src/etl/loaders.py`) : Interface pour charger les donnÃ©es dans PostgreSQL
- âš™ï¸ **Configuration** (`src/utils/config.yaml`) : ParamÃ¨tres essentiels du systÃ¨me

## ğŸ“Š Utilisation

### â–¶ï¸ ExÃ©cution complÃ¨te

Pour lancer le projet complet (ETL + analyse) :

```bash
python run.py
```

### ğŸ”„ Pipeline ETL uniquement

Pour exÃ©cuter uniquement le pipeline ETL :

```bash
python src/etl/pipeline.py
```

### ğŸ“Š Analyses uniquement

Pour gÃ©nÃ©rer seulement les analyses et visualisations :

```bash
python scripts/analyze_data.py
```

### â±ï¸ ExÃ©cution planifiÃ©e

Pour dÃ©marrer le planificateur ETL qui exÃ©cutera le pipeline automatiquement :

```bash
python scripts/schedule_etl.py
```

Le pipeline s'exÃ©cutera:
- âš¡ Une fois immÃ©diatement au lancement du script (pour tester)
- ğŸŒ™ Tous les jours Ã  2h du matin

Vous pouvez modifier la frÃ©quence dans le fichier `scripts/schedule_etl.py`.

## ğŸ“ˆ Monitoring et Performances

### ğŸ“Š Logs et suivi

Le pipeline gÃ©nÃ¨re des logs dÃ©taillÃ©s:
- ğŸ“ `logs/etl_pipeline.log`: Logs du processus ETL
- ğŸ“ `logs/scheduler.log`: Logs du planificateur

Chaque exÃ©cution est Ã©galement enregistrÃ©e dans la table `ETL_Log` de la base de donnÃ©es.

### âš¡ Optimisations de performance

Le pipeline est optimisÃ© pour un traitement efficace:
- ğŸ”„ Chargement par lots pour rÃ©duire les transactions
- ğŸ—œï¸ Compression des donnÃ©es pour amÃ©liorer la vitesse de transfert
- ğŸ” Utilisation d'index pour accÃ©lÃ©rer les requÃªtes
- ğŸ§µ ParallÃ©lisation de certaines opÃ©rations (lorsque possible)

## ğŸ”§ DÃ©pannage

Si vous rencontrez des problÃ¨mes :

1. **ğŸ”´ Erreur de connexion Ã  PostgreSQL**
   - VÃ©rifiez que le service PostgreSQL est actif
   - Confirmez les paramÃ¨tres de connexion dans `src/utils/config.yaml`

2. **âš ï¸ DonnÃ©es manquantes ou incomplÃ¨tes**
   - VÃ©rifiez l'intÃ©gritÃ© des fichiers CSV sources
   - Consultez les logs pour identifier les enregistrements rejetÃ©s

3. **ğŸŒ Performances lentes**
   - VÃ©rifiez les index de la base de donnÃ©es
   - RÃ©duisez la taille des lots si nÃ©cessaire
   - Augmentez les ressources allouÃ©es Ã  PostgreSQL

## ğŸ”® Ã‰volutions futures

Ce projet peut Ãªtre enrichi progressivement avec:

1. ğŸ”„ **Mode incrÃ©mental** : Traitement uniquement des nouvelles donnÃ©es
2. âœ… **Validations avancÃ©es** : ContrÃ´les de qualitÃ© plus poussÃ©s
3. ğŸ“ **Journalisation enrichie** : Suivi plus dÃ©taillÃ© des opÃ©rations
4. ğŸ“Š **Tableau de bord amÃ©liorÃ©** : Visualisations interactives plus complÃ¨tes
5. ğŸ”„ **Flux de travail complexes** : Utilisation d'Apache Airflow pour l'orchestration
6. ğŸ“š **Historisation des donnÃ©es** : Mise en place d'une stratÃ©gie SCD (Slowly Changing Dimensions)

## âœ… Avantages de l'approche

- **ğŸ§© SimplicitÃ©** : Code facile Ã  comprendre et Ã  maintenir
- **ğŸ›¡ï¸ Robustesse** : Gestion d'erreurs amÃ©liorÃ©e pour Ã©viter les interruptions
- **ğŸ§± ModularitÃ©** : Structure permettant d'ajouter facilement de nouvelles fonctionnalitÃ©s
- **âš¡ Performance** : Traitement des donnÃ©es optimisÃ© pour les grandes quantitÃ©s

## ğŸ“œ Licence

Ce projet est sous licence MIT.